import os
import sys
sys.path.append("..")

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from openai import OpenAI, AzureOpenAI

import config as cfg
import my_secrets
from helpers.embedding_helpers import OpenAIEmbeddingFunction

# Environment setup
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

class PolicyRetrieverTool:

    def __init__(self, melding):
        self.melding = melding
        if cfg.summarize_melding_for_policy_retrieval:
            self.melding = self.summarize_melding()
        
        self.db = Chroma(persist_directory=cfg.CHROMA_PATH, 
                         embedding_function=OpenAIEmbeddingFunction())

    def retrieve_policy(self):
        context_text, sources = self.search_db(self.melding)
        prompt = self.create_prompt(context_text)
        response_text = self.invoke_model(prompt)
        formatted_response = self.format_response(response_text, sources)
        return formatted_response

    def summarize_melding(self):
        summarize_prompt = ChatPromptTemplate.from_template(cfg.SUMMARIZE_MELDING_TEMPLATE)
        prompt = summarize_prompt.format(
            melding=self.melding,
        )
        summarized_query = self.invoke_model(prompt, summarize=True)
        
        return summarized_query

    def search_db(self, summarized_melding):
        """
        Search the Chroma database using the summarized melding,
        combining and deduplicating results.

        Args:
            summarized_melding (str): The summarized melding text.

        Returns:
            tuple: A tuple containing:
                - context_text (str): Combined context text from the search results.
                - sources (list): List of sources used in the search.
        """
        summarized_results = (
            self.db.similarity_search_with_score(summarized_melding, k=5) if summarized_melding else []
        )

        combined_results = {
            doc.metadata.get("source"): doc for doc, _ in summarized_results
        }
        sources = combined_results.keys()
        sources = [source.split('/')[-1] for source in sources]
        context_text = "\n\n---\n\n".join([open(os.path.join(cfg.DOCUMENTS_PATH, source), "r").read() for source in sources])
        
        return context_text, sources

    def create_prompt(self, context_text):
        prompt_template = ChatPromptTemplate.from_template(cfg.POLICY_MELDING_TEMPLATE)
        prompt = prompt_template.format(
            context=context_text,
            melding=self.melding,
        )
        return prompt


    def invoke_model(self, prompt, summarize=False):
        """
        Invoke the language model to generate a response based on the provided prompt.

        Args:
            prompt (str): The formatted prompt to be sent to the language model.
            summarize (bool, optional): If True, use a summarizing system message. Defaults to False.

        Returns:
            str: The generated response from the language model.
        """
        system_content = "Je bent een behulpzame assistent" if summarize else "Je bent een behulpzame ambtenaar."

        if cfg.ENDPOINT == 'local':
            client = OpenAI(api_key=my_secrets.API_KEYS["openai"])
        elif cfg.ENDPOINT == 'azure':
            client = AzureOpenAI(
                azure_endpoint=cfg.ENDPOINT_AZURE, 
                api_key=my_secrets.API_KEYS["openai_azure"],  
                api_version="2024-02-15-preview"
            )

        completion = client.chat.completions.create(
            model='gpt-4o-mini',
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt}
            ]
        )

        return completion.choices[0].message.content

    def format_response(self, response_text, sources):
        """
        format the response text and the sources used to generate it.

        Args:
            response_text (str): The text generated by the language model.
            sources (list): List of sources used in generating the response.

        Returns:
            str: The formatted response string with sources included.
        """
        formatted_response = f"Response: {response_text}\nSources: {', '.join(sources)}"
        return formatted_response

if __name__ == '__main__':
    # Specify the melding
    melding = 'Er ligt grofvuil naast een container bij mij in de straat.'

    # Instantiate the PolicyRetrieverTool class with the melding
    fetcher = PolicyRetrieverTool(melding)

    # Retrieve policy
    policy = fetcher.retrieve_policy()
    print(policy)
    