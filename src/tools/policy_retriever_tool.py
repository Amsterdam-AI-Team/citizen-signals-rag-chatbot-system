import os
import sys
sys.path.append("..")

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from openai import OpenAI, AzureOpenAI

import config as cfg
import my_secrets as sc
from helpers.embedding_helpers import OpenAIEmbeddingFunction

# Environment setup
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

class PolicyRetrieverTool:

    def __init__(self, LLM, melding):
        self.llm = LLM
        self.melding = melding
        if cfg.summarize_melding_for_policy_retrieval:
            self.melding = self.summarize_melding()
        
        self.db = Chroma(persist_directory=cfg.CHROMA_PATH, 
                         embedding_function=OpenAIEmbeddingFunction())

    def retrieve_policy(self):
        context_text, sources = self.search_db(self.melding)
        prompt = self.create_prompt(context_text)
        response_text = self.invoke_model(prompt)
        formatted_response = self.format_response(response_text, sources)
        return formatted_response

    def summarize_melding(self):
        summarize_prompt = ChatPromptTemplate.from_template(cfg.SUMMARIZE_MELDING_TEMPLATE)
        prompt = summarize_prompt.format(
            melding=self.melding,
        )
        summarized_query = self.invoke_model(prompt, summarize=True)
        
        return summarized_query

    def search_db(self, summarized_melding):
        """
        Search the Chroma database using the summarized melding,
        combining and deduplicating results.

        Args:
            summarized_melding (str): The summarized melding text.

        Returns:
            tuple: A tuple containing:
                - context_text (str): Combined context text from the search results.
                - sources (list): List of sources used in the search.
        """
        summarized_results = (
            self.db.similarity_search_with_score(summarized_melding, k=5) if summarized_melding else []
        )

        combined_results = {
            doc.metadata.get("source"): doc for doc, _ in summarized_results
        }
        sources = combined_results.keys()
        sources = [source.split('/')[-1] for source in sources]
        context_text = "\n\n---\n\n".join([open(os.path.join(cfg.DOCUMENTS_PATH, source), "r").read() for source in sources])
        
        return context_text, sources

    def create_prompt(self, context_text):
        prompt_template = ChatPromptTemplate.from_template(cfg.POLICY_MELDING_TEMPLATE)
        prompt = prompt_template.format(
            context=context_text,
            melding=self.melding,
        )
        return prompt


    def invoke_model(self, prompt, summarize=False):
        """
        Invoke the language model to generate a response based on the provided prompt.

        Args:
            prompt (str): The formatted prompt to be sent to the language model.
            summarize (bool, optional): If True, use a summarizing system message. Defaults to False.

        Returns:
            str: The generated response from the language model.
        """
        system_content = "Je bent een behulpzame assistent" if summarize else "Je bent een behulpzame ambtenaar."
        response = self.llm.prompt(prompt=prompt, system=system_content)

        return response

    def format_response(self, response_text, sources):
        """
        format the response text and the sources used to generate it.

        Args:
            response_text (str): The text generated by the language model.
            sources (list): List of sources used in generating the response.

        Returns:
            str: The formatted response string with sources included.
        """
        formatted_response = f"Response: {response_text}\nSources: {', '.join(sources)}"
        return formatted_response

if __name__ == '__main__':

    from helpers.llm_helpers import LLMRouter

    LLM = LLMRouter.get_model(
        provider=cfg.provider,
        model_name=cfg.model_name,
        api_endpoint=cfg.AZURE_OPENAI_ENDPOINT,
        api_key=sc.API_KEY,
        api_version=cfg.AZURE_GPT_API_VERSION,
        hf_token=sc.HF_TOKEN,
        hf_cache=cfg.HUGGING_CACHE,
        params=cfg.params,
    )

    # Specify the melding
    melding = 'Er ligt grofvuil naast een container bij mij in de straat.'

    # Instantiate the PolicyRetrieverTool class with the melding
    fetcher = PolicyRetrieverTool(LLM, melding)

    # Retrieve policy
    policy = fetcher.retrieve_policy()
    print(policy)
    