import os
import sys
sys.path.append("..")

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from openai import OpenAI, AzureOpenAI

import config as cfg
from helpers.embedding_helpers import OpenAIEmbeddingFunction

# Environment setup
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

# Fuse to blob storage to access chroma db and policy documents
os.system('sudo blobfuse /home/azureuser/cloudfiles/code/blobfuse/rndaistoragemeldingen --tmp-path=/mnt/resource/blobfusetmp_meldingen --config-file=/home/azureuser/cloudfiles/code/blobfuse/fuse_connection_meldingen.cfg -o attr_timeout=3600 -o entry_timeout=3600 -o negative_timeout=3600 -o allow_other -o nonempty')

class PolicyRetrieverAgent:

    def __init__(self, melding):
        self.melding = melding

    def retrieve_policy(self):
        summarized_melding = self.summarize_melding()
        context_text, sources = self.search_db(summarized_melding)
        prompt = self.create_prompt(context_text)
        response_text = self.invoke_model(prompt)
        formatted_response = self.format_response(response_text, sources)
        return formatted_response

    def summarize_melding(self):
        summarize_prompt = ChatPromptTemplate.from_template(cfg.SUMMARIZE_MELDING_TEMPLATE)
        prompt = summarize_prompt.format(
            melding=self.melding,
        )
        summarized_query = self.invoke_model(prompt, summarize=True)
        
        return summarized_query

    def search_db(self, summarized_melding):
        """
        Search the Chroma database using the summarized melding,
        combining and deduplicating results.

        Args:
            summarized_melding (str): The summarized melding text.

        Returns:
            tuple: A tuple containing:
                - context_text (str): Combined context text from the search results.
                - sources (list): List of sources used in the search.
        """
        db = Chroma(persist_directory=cfg.CHROMA_PATH, embedding_function=OpenAIEmbeddingFunction())

        summarized_results = (
            db.similarity_search_with_score(summarized_melding, k=5) if summarized_melding else []
        )

        combined_results = {
            doc.metadata.get("source"): doc for doc, _ in summarized_results
        }
        sources = combined_results.keys()
        sources = [source.split('/')[-1] for source in sources]
        context_text = "\n\n---\n\n".join([open(os.path.join(cfg.DOCUMENTS_PATH, source), "r").read() for source in sources])
        
        return context_text, sources

    def create_prompt(self, context_text):
        prompt_template = ChatPromptTemplate.from_template(cfg.POLICY_MELDING_TEMPLATE)
        prompt = prompt_template.format(
            context=context_text,
            melding=self.melding,
        )
        return prompt


    def invoke_model(self, prompt, summarize=False):
        """
        Invoke the language model to generate a response based on the provided prompt.

        Args:
            prompt (str): The formatted prompt to be sent to the language model.
            summarize (bool, optional): If True, use a summarizing system message. Defaults to False.

        Returns:
            str: The generated response from the language model.
        """
        system_content = "Je bent een behulpzame assistent" if summarize else "Je bent een behulpzame ambtenaar."

        if cfg.ENDPOINT == 'local':
            client = OpenAI(api_key=cfg.API_KEYS["openai"])
        elif cfg.ENDPOINT == 'azure':
            client = AzureOpenAI(
                azure_endpoint=cfg.ENDPOINT_AZURE, 
                api_key=cfg.API_KEYS["openai_azure"],  
                api_version="2024-02-15-preview"
            )

        completion = client.chat.completions.create(
            model='gpt-4o-mini',
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt}
            ]
        )

        return completion.choices[0].message.content

    def format_response(self, response_text, sources):
        """
        format the response text and the sources used to generate it.

        Args:
            response_text (str): The text generated by the language model.
            sources (list): List of sources used in generating the response.

        Returns:
            str: The formatted response string with sources included.
        """
        formatted_response = f"Response: {response_text}\nSources: {', '.join(sources)}"
        return formatted_response

def main():
    """
    Main function to parse arguments, load session data, process the query, 
    and save the session data.
    """
    melding = 'Er ligt grofvuil naast een container bij mij in de straat.'

    processor = PolicyRetrieverAgent(melding)
    policy = processor.retrieve_policy()
    print(policy)

if __name__ == "__main__":
    main()